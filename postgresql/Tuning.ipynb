{
 "metadata": {
  "name": "",
  "signature": "sha256:03fc29958c060973567cc5721ecf52c32e0996e84a4d4e44e5426868dca415f3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adapted from https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server as of Nov 17, 2014"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pwd\n",
      "sqla_conn = os.getenv('SQLA_CONN') or \\\n",
      "    'postgresql://%s:%s@%s/%s' % (os.getenv('PG_USERNAME') or '', os.getenv('PG_PASSWORD') or '',\n",
      "                                  os.getenv('PG_HOST') or '', \n",
      "                                  os.getenv('PG_DATABASE') or pwd.getpwuid(os.getuid()).pw_name)\n",
      "print(\"Connection is to \" + sqla_conn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext sql "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {sqla_conn}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "param_qry = \"SELECT name, setting, unit, source, short_desc, extra_desc FROM pg_settings WHERE name = '%s'\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Source](https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server \"Permalink to Tuning Your PostgreSQL Server - PostgreSQL wiki\")\n",
      "\n",
      "# Tuning Your PostgreSQL Server - PostgreSQL wiki\n",
      "\n",
      "_by Greg Smith, Robert Treat, and Christopher Browne_\n",
      "\n",
      "PostgreSQL ships with a basic configuration tuned for wide compatibility rather than performance. Odds are good the default parameters are very undersized for your system. Rather than get dragged into the details of everything you should eventually know (which you can find if you want it at the [GUC Three Hour Tour][1]), here we're going to sprint through a simplified view of the basics, with a look at the most common things people new to PostgreSQL aren't aware of. You should click on the name of the parameter in each section to jump to the relevant documentation in the PostgreSQL manual for more details after reading the quick intro here. There is also additional information available about many of these parameters, as well as a list of parameters you shouldn't adjust, at [Server Configuration Tuning][2].\n",
      "\n",
      "##  Background Information on Configuration Settings\n",
      "\n",
      "PostgreSQL settings can be manipulated a number of different ways, but generally you will want to update them in your postgresql.conf file. The specific options available change from release to release, the definitive list is in the source code at src/backend/utils/misc/guc.c for your version of PostgreSQL (but the pg_settings view works well enough for most purposes).\n",
      "\n",
      "###  The types of settings\n",
      "\n",
      "There are several different types of configuration settings, divided up based on the possible inputs they take\n",
      "\n",
      "* Boolean: true, false, on, off\n",
      "* Integer: Whole numbers (2112)\n",
      "* Float: Decimal values (21.12)\n",
      "* Memory / Disk: Integers (2112) or \"computer units\" (512MB, 2112GB). Avoid integers--you need to know the underlying unit to figure out what they mean.\n",
      "* Time: \"Time units\" aka d,m,s (30s). Sometimes the unit is left out; don't do that\n",
      "* Strings: Single quoted text ('pg_log')\n",
      "* ENUMs: Strings, but from a specific list ('WARNING', 'ERROR')\n",
      "* Lists: A comma separated list of strings ('\"$user\",public,tsearch2)\n",
      "\n",
      "###  When they take effect\n",
      "\n",
      "PostgreSQL settings have different levels of flexibility for when they can be changed, usually related to internal code restrictions. The complete list of levels is:\n",
      "\n",
      "* Postmaster: requires restart of server\n",
      "* Sighup: requires a HUP of the server, either by kill -HUP (usually -1), pg_ctl reload, or select pg_reload_conf();\n",
      "* User: can be set within individual sessions, take effect only within that session\n",
      "* Internal: set at compile time, can't be changed, mainly for reference\n",
      "* Backend: settings which must be set before session start\n",
      "* Superuser: can be set at runtime for the server by superusers\n",
      "\n",
      "Most of the time you'll only use the first of these, but the second can be useful if you have a server you don't want to take down, while the user session settings can be helpful for some special situations. You can tell which type of parameter a setting is by looking at the \"context\" field in the pg_settings view.\n",
      "\n",
      "###  Important notes about postgresql.conf\n",
      "\n",
      "* You should be able to find it at $PGDATA/postgresql.conf; watch out for symbolic links and other trickiness\n",
      "* You can figure out the file location with _SHOW config_file_\n",
      "* Lines with # are comments and have no effect. For a new database, this will mean the setting is using the default, but on running systems this may not hold true! Changes to the postgresql.conf do not take effect without a reload/restart, so it's possible for the system to be running something different from what is in the file.\n",
      "* If the same setting is listed multiple times, the last one wins.\n",
      "\n",
      "###  Viewing the current settings\n",
      "\n",
      "* Look in postgresql.conf. This works if you follow good practice, but it's not definitive!\n",
      "* _show all_, _show _ will show you the current value of the setting. Watch out for session specific changes\n",
      "* _select * from pg_settings_ will label session specific changes as locally modified"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### listen_addresses\n",
      "\n",
      "By default, PostgreSQL only responds to connections from the local host. If you want your server to be accessible from other systems via standard TCP/IP networking, you need to change listen_addresses from its default. The usual approach is to set it to listen to all addresses like this:\n",
      "\n",
      "`\n",
      "\n",
      "\n",
      "    listen_addresses = '*'\n",
      "\n",
      "\n",
      "`\n",
      "\n",
      "And then control who can and cannot connect via the [pg_hba.conf][3] file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'listen_addresses'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### max_connections\n",
      "\n",
      "max_connections sets exactly that: the maximum number of client connections allowed. This is very important to some of the below parameters (particularly work_mem) because there are some memory resources that are or can be allocated on a per-client basis, so the maximum number of clients suggests the maximum possible memory use. Generally, PostgreSQL on good hardware can support a few hundred connections. If you want to have thousands instead, you should consider using [connection pooling software][4] to reduce the connection overhead."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'max_connections'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### shared_buffers\n",
      "\n",
      "The shared_buffers configuration parameter determines how much memory is dedicated to PostgreSQL to use for caching data. One reason the defaults are low is because on some platforms (like older Solaris versions and SGI), having large values requires invasive action like recompiling the kernel. Even on a modern Linux system, the stock kernel will likely not allow setting shared_buffers to over 32MB without adjusting kernel settings first. (PostgreSQL 9.3 and later use a different shared memory mechanism, so kernel settings will usually not have to be adjusted there.)\n",
      "\n",
      "If you have a system with 1GB or more of RAM, a reasonable starting value for shared_buffers is 1/4 of the memory in your system. If you have less RAM you'll have to account more carefully for how much RAM the OS is taking up; closer to 15% is more typical there. There are some workloads where even larger settings for shared_buffers are effective, but given the way PostgreSQL also relies on the operating system cache, it's unlikely you'll find using more than 40% of RAM to work better than a smaller amount.\n",
      "\n",
      "Be aware that if your system or PostgreSQL build is 32-bit, it might not be practical to set shared_buffers above 2 ~ 2.5GB. See [this blog post][5] for details.\n",
      "\n",
      "Note that on Windows, large values for shared_buffers aren't as effective, and you may find better results keeping it relatively low and using the OS cache more instead. On Windows the useful range is 64MB to 512MB.\n",
      "\n",
      "If you are running PostgreSQL 9.2 or earlier, it's likely you will have to increase the amount of memory your operating system allows you to allocate at once to set the value for shared_buffers this high. On UNIX-like systems, if you set it above what's supported, you'll get a message like this:\n",
      "\n",
      "`\n",
      "\n",
      "\n",
      "    IpcMemoryCreate: shmget(key=5432001, size=415776768, 03600) failed: Invalid argument\n",
      "\n",
      "    This error usually means that PostgreSQL's request for a shared memory\n",
      "    segment exceeded your kernel's SHMMAX parameter. You can either\n",
      "    reduce the request size or reconfigure the kernel with larger SHMMAX.\n",
      "    To reduce the request size (currently 415776768 bytes), reduce\n",
      "    PostgreSQL's shared_buffers parameter (currently 50000) and/or\n",
      "    its max_connections parameter (currently 12).\n",
      "\n",
      "\n",
      "`\n",
      "\n",
      "See [Managing Kernel Resources][6] for details on how to correct this.\n",
      "\n",
      "Changing this setting requires restarting the database. Also, this is a hard allocation of memory; the whole thing gets allocated out of virtual memory when the database starts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'shared_buffers'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### effective_cache_size\n",
      "\n",
      "effective_cache_size should be set to an estimate of how much memory is available for disk caching by the operating system and within the database itself, after taking into account what's used by the OS itself and other applications. This is a guideline for how much memory you expect to be available in the OS and PostgreSQL buffer caches, not an allocation! This value is used only by the PostgreSQL query planner to figure out whether plans it's considering would be expected to fit in RAM or not. If it's set too low, indexes may not be used for executing queries the way you'd expect. The setting for shared_buffers is not taken into account here--only the effective_cache_size value is, so it should include memory dedicated to the database too.\n",
      "\n",
      "Setting effective_cache_size to 1/2 of total memory would be a normal conservative setting, and 3/4 of memory is a more aggressive but still reasonable amount. You might find a better estimate by looking at your operating system's statistics. On UNIX-like systems, add the free+cached numbers from free or top to get an estimate. On Windows see the \"System Cache\" size in the Windows Task Manager's Performance tab. Changing this setting does not require restarting the database (HUP is enough)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'effective_cache_size'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### checkpoint_segments checkpoint_completion_target\n",
      "\n",
      "PostgreSQL writes new transactions to the database in files called WAL segments that are 16MB in size. Every time checkpoint_segments worth of these files have been written, by default 3, a checkpoint occurs. Checkpoints can be resource intensive, and on a modern system doing one every 48MB will be a serious performance bottleneck. Setting checkpoint_segments to a much larger value improves that. Unless you're running on a very small configuration, you'll almost certainly be better setting this to at least 10, which also allows usefully increasing the completion target.\n",
      "\n",
      "For more write-heavy systems, values from 32 (checkpoint every 512MB) to 256 (every 4GB) are popular nowadays. Very large settings use a lot more disk and will cause your database to take longer to recover, so make sure you're comfortable with both those things before large increases. Normally the large settings (>64/1GB) are only used for bulk loading. Note that whatever you choose for the segments, you'll still get a checkpoint at least every 5 minutes unless you also increase checkpoint_timeout (which isn't necessary on most systems).\n",
      "\n",
      "Checkpoint writes are spread out a bit while the system starts working toward the next checkpoint. You can spread those writes out further, lowering the average write overhead, by increasing the checkpoint_completion_target parameter to its useful maximum of 0.9 (aim to finish by the time 90% of the next checkpoint is here) rather than the default of 0.5 (aim to finish when the next one is 50% done). A setting of 0 gives something similar to the behavior of obsolete versions. The main reason the default isn't just 0.9 is that you need a larger checkpoint_segments value than the default for broader spreading to work well. For lots more information on checkpoint tuning, see [Checkpoints and the Background Writer][7] (where you'll also learn why tuning the background writer parameters is challenging to do usefully)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'checkpoint_segments'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'checkpoint_completion_target'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### autovacuum\n",
      "\n",
      "The autovacuum process takes care of several maintenance chores inside your database that you really need. Generally, if you think you need to turn regular vacuuming off because it's taking too much time or resources, that means you're doing it wrong. The answer to almost all vacuuming problems is to vacuum more often, not less, so that each individual vacuum operation has less to clean up.\n",
      "\n",
      "However, it's acceptable to disable autovacuum for short periods of time, for instance when bulk loading large amounts of data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'autovacuum'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### logging\n",
      "\n",
      "There are many things you can log that may or may not be important to you. You should investigate the documentation on all of the options, but here are some tips & tricks to get you started:\n",
      "\n",
      "* pgFouine is a tool used to analyze postgresql logs for performance tuning. If you plan to use this tool, it has specific logging requirements. Please check http://pgfouine.projects.postgresql.org/\n",
      "* pgFouine has been obsoleted by PgBadger\n",
      "* [PgCluu][8] is an handy tool from the author of PgBadger, and is a PostgreSQL performances monitoring and auditing tool.\n",
      "* log_destination & log_directory (& log_filename): What you set these options to is not as important as knowing they can give you hints to determine where your database server is logging to. Best practice would be to try and make this as similar as possible across your servers. Note that in some cases, the init script starting your database may be customizing the log destination in the command line used to start the database, overriding what's in the postgresql.conf (and making it so you'll get different behavior if you run pg_ctl manually instead of using the init script).\n",
      "* log_min_error_statement: You should probably make sure this is at least on error, so that you will see any SQL commands which cause an error. should be the default on recent versions.\n",
      "* log_min_duration_statement: Not necessary for everyday use, but this can generate [logs of \"slow queries\"][9] on your system.\n",
      "* log_line_prefix: Appends information to the start of each line. A good generic recommendation is '%t:%r:%u@%d:[%p]: ' : %t=timestamp, %u=db user name, %r=host connecting from, %d=database connecting to, %p=PID of connection. It may not be obvious what the PID is useful at first, but it can be vital for trying to troubleshoot problems in the future so better to put in the logs from the start.\n",
      "* log_statement: Choices of none, ddl, mod, all. Using all in production leads to severe performance penalties. DDL can sometime be helpful to discover rogue changes made outside of your recommend processes, by \"cowboy DBAs\" for example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'logging'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### default_statistics_target\n",
      "\n",
      "The database software collects statistics about each of the tables in your database to decide how to execute queries against it. If you're not getting good execution query plans particularly on larger (or more varied) tables you should increase default_statistics_target then ANALYZE the database again (or wait for autovacuum to do it for you).\n",
      "\n",
      "##### PostgreSQL 8.4 and later\n",
      "\n",
      "The starting default_statistics_target value was raised from 10 to 100 in PostgreSQL 8.4. Increases beyond 100 may still be useful, but this increase makes for greatly improved statistics estimation in the default configuration. The maximum value for the parameter was also increased from 1000 to 10,000 in 8.4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'default_statistics_target'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### work_mem\n",
      "\n",
      "If you do a lot of complex sorts, and have a lot of memory, then increasing the `work_mem` parameter allows PostgreSQL to do larger in-memory sorts which, unsurprisingly, will be faster than disk-based equivalents.\n",
      "\n",
      "This size is applied to each and every sort done by each user, and complex queries can use multiple working memory sort buffers. Set it to 50MB, and have 30 users submitting queries, and you are soon using 1.5GB of real memory. Furthermore, if a query involves doing merge sorts of 8 tables, that requires 8 times work_mem. You need to consider what you set max_connections to in order to size this parameter correctly. This is a setting where data warehouse systems, where users are submitting very large queries, can readily make use of many gigabytes of memory.\n",
      "\n",
      "[log_temp_files][10] can be used to log sorts, hashes, and temp files which can be useful in figuring out if sorts are spilling to disk instead of fitting in memory. You can see sorts spilling to disk using `EXPLAIN ANALYZE` plans as well. For example, if you see a line like `Sort Method: external merge Disk: 7526kB` in the output of EXPLAIN ANALYZE, a `work_mem` of at least 8MB would keep the intermediate data in memory and likely improve the query response time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'work_mem'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### maintenance_work_mem\n",
      "\n",
      "Specifies the maximum amount of memory to be used by maintenance operations, such as VACUUM, CREATE INDEX, and ALTER TABLE ADD FOREIGN KEY. It defaults to 16 megabytes (16MB). Since only one of these operations can be executed at a time by a database session, and an installation normally doesn't have many of them running concurrently, it's safe to set this value significantly larger than work_mem. Larger settings might improve performance for vacuuming and for restoring database dumps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'maintenance_work_mem'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### wal_sync_method wal_buffers\n",
      "\n",
      "After every transaction, PostgreSQL forces a commit to disk out to its write-ahead log. This can be done a couple of ways, and on some platforms the other options are considerably faster than the conservative default. open_sync is the most common non-default setting switched to, on platforms that support it but default to one of the fsync methods. See [Tuning PostgreSQL WAL Synchronization][11] for a lot of background on this topic. Note that open_sync writing is buggy on some platforms (such as [Linux][12]), and you should (as always) do plenty of tests under a heavy write load to make sure that you haven't made your system less stable with this change. [Reliable Writes][13] contains more information on this topic.\n",
      "\n",
      "Linux kernels starting with version 2.6.33 will cause earlier versions of PostgreSQL to default to wal_sync_method=open_datasync; before that kernel release the default picked was always fdatasync. This can cause a significant performance decrease when combined with small writes and/or small values for wal_buffers.\n",
      "\n",
      "Increasing wal_buffers from its tiny default of a small number of kilobytes is helpful for write-heavy systems. Benchmarking generally suggests that just increasing to 1MB is enough for some large systems, and given the amount of RAM in modern servers allocating a full WAL segment (16MB, the useful upper-limit here) is reasonable. Changing wal_buffers requires a database restart.\n",
      "\n",
      "##### PostgreSQL 9.1 and later\n",
      "\n",
      "Starting with PostgreSQL 9.1 wal_buffers defaults to being 1/32 of the size of shared_buffers, with an upper limit of 16MB (reached when shared_buffers=512MB).\n",
      "\n",
      "PostgreSQL 9.1 also changes the logic for selecting the default wal_sync_method such that on newer Linux kernels, it will still select fdatasync as its method--the same as on older Linux versions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'wal_sync_method'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'wal_buffers'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### constraint_exclusion\n",
      "\n",
      "`constraint_exclusion` now defaults to a new choice: `partition`. This will only enable constraint exclusion for partitioned tables which is the right thing to do in nearly all cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'constraint_exclusion'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### max_prepared_transactions\n",
      "\n",
      "This setting is used for managing 2 phase commit. If you do not use two phase commit (and if you don't know what it is, you don't use it), then you can set this value to 0. That will save a little bit of shared memory. For database systems with a large number (at least hundreds) of concurrent connections, be aware that this setting also affects the number of available lock-slots in pg_locks, so you may want to leave it at the default setting. There is a formula for how much memory gets allocated [in the docs][14] and in the default postgresql.conf.\n",
      "\n",
      "Changing max_prepared_transactions requires a server restart."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'max_prepared_transactions'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### synchronous_commit\n",
      "\n",
      "PostgreSQL can only safely use a write cache if it has a battery backup. See [WAL reliability][15] for an essential introduction to this topic. No, really; go read that right now, it's vital to understand that if you want your database to work right.\n",
      "\n",
      "You may be limited to approximately 100 transaction commits per second per client in situations where you don't have such a durable write cache (and perhaps only 500/second even with lots of clients).\n",
      "\n",
      "For situations where a small amount of data loss is acceptable in return for a large boost in how many updates you can do to the database per second, consider switching synchronous commit off. This is particularly useful in the situation where you do not have a battery-backed write cache on your disk controller, because you could potentially get thousands of commits per second instead of just a few hundred.\n",
      "\n",
      "For obsolete versions of PostgreSQL, you may find people recommending that you set _fsync=off_ to speed up writes on busy systems. This is dangerous--a power loss could result in your database getting corrupted and not able to start again. Synchronous commit doesn't introduce the risk of _corruption_, which is really bad, just some risk of data _loss_."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'synchronous_commit'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### random_page_cost\n",
      "\n",
      "This setting suggests to the optimizer how long it will take your disks to seek to a random disk page, as a multiple of how long a sequential read (with a cost of 1.0) takes. If you have particularly fast disks, as commonly found with RAID arrays of SCSI disks, it may be appropriate to lower random_page_cost, which will encourage the query optimizer to use random access index scans. Some feel that 4.0 is always too large on current hardware; it's not unusual for administrators to standardize on always setting this between 2.0 and 3.0 instead. In some cases that behavior is a holdover from earlier PostgreSQL versions where having random_page_cost too high was more likely to screw up plan optimization than it is now (and setting at or below 2.0 was regularlly necessary). Since these cost estimates are just that--estimates--it shouldn't hurt to try lower values.\n",
      "\n",
      "But this not where you should start to search for plan problems. Note that random_page_cost is pretty far down this list (at the end in fact). If you are getting bad plans, this shouldn't be the first thing you look at, even though lowering this value may be effective. Instead, you should start by making sure autovacuum is working properly, that you are collecting enough statistics, and that you have correctly sized the memory parameters for your server--all the things gone over above. After you've done all those much more important things, if you're still getting bad plans _then_ you should see if lowering random_page_cost is still useful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%sql {param_qry % 'random_page_cost'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1]: http://www.pgcon.org/2008/schedule/events/104.en.html\n",
      "[2]: https://www.packtpub.com/article/server-configuration-tuning-postgresql\n",
      "[3]: http://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html\n",
      "[4]: https://wiki.postgresql.org/wiki/Replication%2C_Clustering%2C_and_Connection_Pooling \"Replication, Clustering, and Connection Pooling\"\n",
      "[5]: http://rhaas.blogspot.jp/2011/05/sharedbuffers-on-32-bit-systems.html\n",
      "[6]: http://www.postgresql.org/docs/current/static/kernel-resources.html\n",
      "[7]: http://www.westnet.com/~gsmith/content/postgresql/chkp-bgw-83.htm\n",
      "[8]: https://github.com/darold/pgcluu\n",
      "[9]: https://wiki.postgresql.org/wiki/Logging_Difficult_Queries \"Logging Difficult Queries\"\n",
      "[10]: http://www.postgresql.org/docs/9.3/static/runtime-config-logging.html#GUC-LOG-TEMP-FILES\n",
      "[11]: http://www.westnet.com/~gsmith/content/postgresql/TuningPGWAL.htm\n",
      "[12]: http://lwn.net/Articles/350219/\n",
      "[13]: https://wiki.postgresql.org/wiki/Reliable_Writes \"Reliable Writes\"\n",
      "[14]: http://www.postgresql.org/docs/current/static/kernel-resources.html#SHARED-MEMORY-PARAMETERS\n",
      "[15]: http://www.postgresql.org/docs/current/static/wal-reliability.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}